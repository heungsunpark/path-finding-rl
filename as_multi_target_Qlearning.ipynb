{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","__file__ = '/content/drive/MyDrive/aiffelthon/data'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-B1raELdgTz","executionInfo":{"status":"ok","timestamp":1652788229719,"user_tz":-540,"elapsed":18411,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"}},"outputId":"4263b56b-ef4e-4d3c-cd59-f8d5a96366f8"},"id":"0-B1raELdgTz","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"154f8442","metadata":{"id":"154f8442"},"source":["## Environment"]},{"cell_type":"code","execution_count":4,"id":"e672c2e2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e672c2e2","executionInfo":{"status":"ok","timestamp":1652788239864,"user_tz":-540,"elapsed":1551,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"}},"outputId":"c458d1f5-e2c0-4fbf-9f16-fb52fc0e4750"},"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -0.1 -0.1 100\n"]}],"source":["from string import ascii_uppercase\n","#from draw_utils import *\n","#from pyglet.gl import *\n","import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from datetime import datetime\n","import pytz\n","import matplotlib.pyplot as plt\n","\n","# reward\n","move_reward = -0.1\n","obs_reward = -0.1\n","goal_reward = 100\n","\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","#__file__ = '/home/ogangza/heung_path_finding/path-finding-rl/data'\n","local_path = os.path.abspath(os.path.join(os.path.dirname(__file__)))\n","\n","class Simulator:\n","    def __init__(self):\n","        '''\n","        height : 그리드 높이\n","        width : 그리드 너비 \n","        inds : A ~ Q alphabet list\n","        '''\n","        # Load train data\n","        self.files = pd.read_csv(os.path.join(local_path, \"data/factory_order_train.csv\"))\n","        self.height = 10\n","        self.width = 9\n","        self.inds = list(ascii_uppercase)[:17]\n","\n","    def set_box(self):\n","        '''\n","        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n","        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n","        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n","        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n","        '''\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","        ######################################\n","        #print('box data:', box_data)\n","        ######################################\n","        # 물건이 들어있을 수 있는 경우\n","        for box in box_data.itertuples(index = True, name ='Pandas'): #판다스 데이터프레임을 행단위로 반복 처리\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 100\n","            #######################################\n","            #print('self.grid:', self.grid)\n","            #######################################\n","        # 물건이 실제 들어있는 경우\n","        order_item = list(set(self.inds) & set(self.items))  # 에피소드에 해당하는 items는 reset 메서드에서 결정\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        ########################################\n","        #print('order_csv:', order_csv)\n","        ########################################\n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100\n","            # local target에 가야 할 위치 좌표 넣기\n","            self.local_target.append(\n","                [getattr(order_box, \"row\"),\n","                 getattr(order_box, \"col\")]\n","                )\n","\n","            #######################################\n","            #print('self.grid:', self.grid)\n","            #######################################\n","\n","        ################################################\n","        #print('self.local_target.sort() 전:', self.local_target)\n","        ################################################\n","        #self.local_target.sort()   # 인풋 데이터 파일이 소팅되어 있으므로 그대로 사용. 2차원 배열 소팅이므로 A, B, C순으로 정렬 안됨\n","        self.local_target.append([9,4]) # 최종 목적지 (출발점) 추가\n","        ################################################\n","        #print('self.local_target.sort():', self.local_target)\n","        ###############################################\n","        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n","\n","    def set_obstacle(self):\n","        '''\n","        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n","        '''\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n","            \n","        ##########################################\n","        #print('self.grid:', self.grid)\n","        ##########################################\n","        \n","    def reset(self, epi):\n","        '''\n","        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n","         - param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n","         - return: 초기 셋팅된 그리드\n","         - rtype: numpy.ndarray\n","        _____________________________________________________________________________________\n","         - items: 이번 에피소드에서 가져와야 하는 아이템들\n","         - terminal_location: 현재 에이전트가 찾아가야 하는 목적지(item). 중간 경유지\n","         - local_target: 한 에피소드에서 찾아가야 하는 중간 경유지와 최종 목적지의 위치 (좌표)\n","         - actions: visualization을 위해 에이전트 action을 저장하는 리스트. 이동 좌표들의 리스트\n","         - curloc: 현재 위치\n","        '''\n","        # initial episode parameter setting\n","        self.epi = epi\n","        self.items = list(self.files.iloc[self.epi])[0]  # 해당 에피소드의 items를 가져 옴. 예, [ 'H', 'L', 'M']\n","        self.cumulative_reward = 0\n","        self.terminal_location = None\n","        self.local_target = []\n","        self.actions = []\n","\n","        # initial grid setting\n","        self.grid = np.ones((self.height, self.width), dtype=\"float16\")\n","\n","        # set information about the gridworld\n","        self.set_box()  # 에이전트가 이번에 방문해야 할 좌표들 저장. 예, [ [0,3], [0,7], [0,8], [9,4] ]\n","        self.set_obstacle()\n","\n","        # start point를 grid에 표시\n","        self.curloc = [9, 4]  # 에피소드의 출발점\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5\n","        \n","        self.done = False\n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):\n","        '''\n","        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n","        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n","        \n","        :param x: 에이전트의 현재 x 좌표\n","        :param y: 에이전트의 현재 y 좌표\n","        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n","        :rtype: int, int\n","        '''\n","        new_x = cur_x\n","        new_y = cur_y\n","        # up\n","        if action == 0:\n","            new_x = cur_x - 1\n","        # down\n","        elif action == 1:\n","            new_x = cur_x + 1\n","        # left\n","        elif action == 2:\n","            new_y = cur_y - 1\n","        # right\n","        else:\n","            new_y = cur_y + 1\n","\n","        return int(new_x), int(new_y)\n","\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):\n","        '''\n","        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n","         - new_x: action에 따른 에이전트 새로운 위치 좌표 x\n","         - new_y: action에 따른 에이전트 새로운 위치 좌표 y\n","         - out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n","         - return: action에 따른 리워드\n","         - rtype: float\n","        '''\n","\n","        # 바깥으로 나가는 경우\n","        if any(out_of_boundary):\n","            reward = obs_reward\n","                       \n","        else:\n","            # 장애물에 부딪히는 경우 \n","            if self.grid[new_x][new_y] == 0:\n","                reward = obs_reward  \n","\n","            # 현재 목표에 도달한 경우\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","                reward = goal_reward\n","\n","            # 그냥 움직이는 경우 \n","            else:\n","                reward = move_reward\n","\n","        return reward\n","\n","    def step(self, action):\n","        ''' \n","        에이전트의 action에 따라 step을 진행한다.\n","        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n","        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n","\n","         - action: 에이전트 행동\n","         - return: grid(그리드), reward(리워드), cumulative_reward(누적 리워드), done(최종 목적지 도달 여부),\n","             goal_ob_reward(goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터)\n","         - rtype: numpy.ndarray, float, float, bool, bool/str\n","        (Hint: 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n","        '''\n","        self.terminal_location = self.local_target[0]  # 현재의 목적지 확인 (또는 설정) \n","        cur_x,cur_y = self.curloc  # 현재 위치\n","\n","        self.actions.append((cur_x, cur_y))  # 현재 위치를 actions 리스트에 추가\n","\n","        goal_ob_reward = False\n","        \n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)  # 현 위치에서 인풋으로 받은 action 적용해서 새 위치 구함\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]  # OB 판정. OB면 True\n","\n","        # 바깥으로 나가는 경우 종료\n","        if any(out_of_boundary):\n","            #원 위치\n","            new_x = cur_x\n","            new_y = cur_y\n","            #self.done = True\n","            goal_ob_reward = True\n","        else:\n","            # 장애물에 부딪히는 경우 종료\n","            if self.grid[new_x][new_y] == 0:\n","                #원 위치\n","                new_x = cur_x\n","                new_y = cur_y\n","                #self.done = True\n","                goal_ob_reward = True\n","            # 현재 목표에 도달한 경우, 다음 목표 설정\n","            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n","                # end point 일 때\n","                if [new_x, new_y] == [9,4]:\n","                    self.done = True\n","                # 현재 목표에 도달한 모든 경우에 대해\n","                self.local_target.remove(self.local_target[0])\n","                self.grid[cur_x][cur_y] = 1\n","                self.grid[new_x][new_y] = -5\n","                goal_ob_reward = True\n","                self.curloc = [new_x, new_y]\n","            # 그냥 움직이는 경우 \n","            else:\n","                self.grid[cur_x][cur_y] = 1\n","                self.grid[new_x][new_y] = -5\n","                self.curloc = [new_x,new_y]\n","                \n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        self.cumulative_reward += reward\n","        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward\n","\n"]},{"cell_type":"markdown","id":"32f6115a","metadata":{"id":"32f6115a"},"source":["## Agent"]},{"cell_type":"code","execution_count":5,"id":"a97ebaaf","metadata":{"id":"a97ebaaf","executionInfo":{"status":"ok","timestamp":1652788247929,"user_tz":-540,"elapsed":333,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"}}},"outputs":[],"source":["class QAgent():\n","    def __init__(self):\n","        self.height = 10\n","        self.width = 9\n","        self.q_table = np.zeros((self.height, self.width, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n","        self.eps = 0.9\n","\n","    def select_action(self, s): \n","        # eps-greedy로 액션을 선택해준다\n","        x, y = s\n","        '''\n","        # 출발점에서는 무조건 위로 올라간다 (action=0)\n","        if [x,y] == [9,4]:\n","            action = 0\n","            return action\n","        \n","        # 위쪽 Rack에 들어가면 항상 아래로 나온다 (action=1)\n","        upper_rack = [[0,0],[0,1],[0,2],[0,3],[0,4],[0,5],[0,6],[0,7],[0,8]]\n","        if [x,y] in upper_rack:\n","            action = 1\n","            return action\n","        \n","        # 왼쪽 Rack에 들어가면 항상 오른쪽으로 나온다 (action=3)\n","        left_rack = [[2,0],[3,0],[4,0],[5,0]]\n","        if [x,y] in left_rack:\n","            action = 3\n","            return action\n","        \n","        # 오른쪽 Rack에 들어가면 항상 왼쪽으로 나온다 (action=2)\n","        right_rack = [[2,8],[3,8],[4,8],[5,8]]\n","        if [x,y] in right_rack:\n","            action = 2\n","            return action\n","        '''\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = self.q_table[x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition):\n","        s, a, r, s_prime = transition\n","        x,y = s\n","        next_x, next_y = s_prime\n","        # Q러닝 업데이트 식을 이용 \n","        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a])\n","\n","    def anneal_eps(self):\n","        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함.\n","        self.eps = max(self.eps, 0.2) \n","\n","    def show_table(self):\n","        q_lst = self.q_table.tolist()\n","        data = np.zeros((self.height, self.width))\n","        for row_idx in range(len(q_lst)):\n","            row = q_lst[row_idx]\n","            for col_idx in range(len(row)):\n","                col = row[col_idx]\n","                action = np.argmax(col)\n","                data[row_idx, col_idx] = action\n","        print(data)\n","        \n","        #좌, 상, 우, 하 = 2, 0, 3, 1\n","        data_direction = np.where(data == 2., '←', data)\n","        data_direction = np.where(data == 0., '↑', data_direction)\n","        data_direction = np.where(data == 3., '→', data_direction)\n","        data_direction = np.where(data == 1., '↓', data_direction)\n","\n","        data_direction[9,4] = 'S'\n","        data_direction[9,4] = 'G'\n","        \n","        # 맨 아래 줄\n","        for i in range(4):\n","            data_direction[9,i] = '■'\n","        for i in range(5,9):\n","            data_direction[9,i] = '■'\n","        \n","        # 맨 윗 줄\n","        upper_line = 'ⓔⓕⓖⓗⓘⓙⓚⓛⓜ'\n","        for i in range(9):\n","            data_direction[0,i] = upper_line[i]\n","        \n","        # 왼쪽, 오른쪽 줄\n","        left_line = 'ⓓⓒⓑⓐ'\n","        right_line = 'ⓝⓞⓟⓠ'\n","        for i in range(2,6):\n","            data_direction[i,0] = left_line[i-2]\n","            data_direction[i,8] = right_line[i-2]\n","            \n","        # 중간 장애물\n","        for i in range(3,7):\n","            data_direction[i,2] = '■'\n","            data_direction[i,4] = '■'\n","            data_direction[i,6] = '■'\n","        print(data_direction)"]},{"cell_type":"markdown","id":"5e284183","metadata":{"id":"5e284183"},"source":["### Qlearning Main"]},{"cell_type":"code","execution_count":16,"id":"4ce9be70","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"4ce9be70","executionInfo":{"status":"ok","timestamp":1652791376062,"user_tz":-540,"elapsed":1394872,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"}},"outputId":"4e353938-5ffc-4656-b5f3-607bab58e037"},"outputs":[{"output_type":"stream","name":"stdout","text":["list(env.files.iloc[0])[0]: ['H', 'L', 'M']\n","▶ Episode # 0 Iteration # 0 → 경로 길이: 292\n","▶ Episode # 0 Iteration # 1000 → 경로 길이: 1295\n","▶ Episode # 0 Iteration # 2000 → 경로 길이: 1726\n","▶ Episode # 0 Iteration # 3000 → 경로 길이: 1161\n","▶ Episode # 0 Iteration # 4000 → 경로 길이: 1166\n","▶ Episode # 0 Iteration # 5000 → 경로 길이: 1926\n","▶ Episode # 0 Iteration # 6000 → 경로 길이: 2043\n","▶ Episode # 0 Iteration # 7000 → 경로 길이: 1247\n","▶ Episode # 0 Iteration # 8000 → 경로 길이: 13047\n","▶ Episode # 0 Iteration # 9000 → 경로 길이: 28117\n","[[3. 3. 3. 1. 2. 2. 3. 2. 2.]\n"," [3. 3. 3. 1. 3. 3. 0. 0. 0.]\n"," [3. 3. 0. 1. 3. 0. 0. 2. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [3. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [1. 1. 0. 1. 0. 0. 0. 1. 1.]\n"," [3. 3. 3. 0. 1. 3. 1. 2. 2.]\n"," [3. 3. 3. 3. 1. 1. 1. 2. 2.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n","[['ⓔ' 'ⓕ' 'ⓖ' 'ⓗ' 'ⓘ' 'ⓙ' 'ⓚ' 'ⓛ' 'ⓜ']\n"," ['→' '→' '→' '↓' '→' '→' '↑' '↑' '↑']\n"," ['ⓓ' '→' '↑' '↓' '→' '↑' '↑' '←' 'ⓝ']\n"," ['ⓒ' '↑' '■' '↓' '■' '↑' '■' '↑' 'ⓞ']\n"," ['ⓑ' '↑' '■' '↓' '■' '↑' '■' '↑' 'ⓟ']\n"," ['ⓐ' '↑' '■' '↓' '■' '↑' '■' '↑' 'ⓠ']\n"," ['↓' '↓' '■' '↓' '■' '↑' '■' '↓' '↓']\n"," ['→' '→' '→' '↑' '↓' '→' '↓' '←' '←']\n"," ['→' '→' '→' '→' '↓' '↓' '↓' '←' '←']\n"," ['■' '■' '■' '■' 'G' '■' '■' '■' '■']]\n"]}],"source":["tz = pytz.timezone('Asia/Seoul')\n","env = Simulator()\n","agent = QAgent()\n","print('list(env.files.iloc[0])[0]:', list(env.files.iloc[0])[0])\n","\n","for n_epi in range(1): # range()의 인수로 len(env.files) 사용하면 됨 (=39,999)\n","    obs = env.reset(n_epi)\n","    path = env.local_target\n","\n","    for iter in range(10000): # 하나의 에피소드에 대해서 최적 경로를 구하기 위한 반복\n","\n","        done = False\n","        obs = env.reset(n_epi) # 이번 에피소드에서 방문해야 할 좌표 저장 @ env.local_target 리스트\n","        s = env.curloc\n","\n","        while not done:  # 최종 목표에 도달하면 1회 끝남\n","            a = agent.select_action(s) # e-greedy로 액션 선택\n","            obs, r, cum_reward, done, goal_ob_reward = env.step(a)  # 중간 목표 도달시 다음 목표로 자동 바뀜 발생\n","            s_prime = env.curloc  # 새 위치 저장\n","            agent.update_table((s, a, r, s_prime))\n","            s = s_prime\n","            # while loop 종료\n","\n","        if iter % 1000 == 0:\n","            #cur_time = datetime.now(tz)\n","            #simple_cur_time = cur_time.strftime(\"%H:%M:%S\")\n","            print('▶ Episode #', n_epi, 'Iteration #', iter, end=' → ')\n","            #print('start time:', simple_cur_time, end=' → ')\n","            #print('cum_reward =', cum_reward, 'appended actions =', s, end='...')\n","            print('경로 길이:', len(env.actions))\n","\n","        agent.anneal_eps()\n","        # for_iter loop 종료\n","    agent.show_table()\n","    # for_episode loop 종료\n"]},{"cell_type":"code","source":["# 최적 경로\n","x = 9\n","y = 4\n","optimal_path = [[9,4]]\n","print('path:', path)\n","print(len(path))\n","\n","while len(path) > 0:\n","    action_val = agent.q_table[x,y,:]\n","    action = np.argmax(action_val)\n","    print('action:', action)\n","    new_x, new_y = env.apply_action(action, x, y)\n","    x = new_x\n","    y = new_y\n","    print('[',x,y,']', end = ' ')\n","    optimal_path.append([x,y])\n","    if [x, y] == path[0]:\n","        if [x,y] == [9,4]:\n","            print('arrived [9,4]. finished!')\n","            break\n","        else:\n","            print('arrived ', path[0])\n","            path.remove(path[0])\n","\n","print('Optimal path:', optimal_path)"],"metadata":{"id":"jdM5cOii3_uO","colab":{"base_uri":"https://localhost:8080/","height":318},"executionInfo":{"status":"error","timestamp":1652794010836,"user_tz":-540,"elapsed":349,"user":{"displayName":"heungsun 박흥선 park","userId":"16802321133888950576"}},"outputId":"fea36f83-f927-4054-e41c-c778e5e115bd"},"id":"jdM5cOii3_uO","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["path: [[0, 3], [0, 7], [0, 8], [9, 4]]\n","4\n","action: 1\n","[ 10 4 ] "]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/aiffelthon/data\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maction_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"]}]},{"cell_type":"markdown","source":["     39         optimal_path = [[9,4]]\n","     40         while len(path) > 0:\n","---> 41             action_val = agent.q_table[x,y,:]\n","     42             action = np.argmax(action_val)\n","     43             new_x, new_y = env.apply_action(action, x, y)\n","\n","IndexError: index 10 is out of bounds for axis 0 with size 10"],"metadata":{"id":"paeomX_a5yOu"},"id":"paeomX_a5yOu"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"as_multi_target_Qlearning.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}